{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"jDIyOoG87p5R","executionInfo":{"status":"ok","timestamp":1712448311111,"user_tz":-330,"elapsed":1,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}}},"outputs":[],"source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","import nltk.stem as stemmer\n","from nltk.stem import *\n","from nltk.stem.porter import *\n","stemmer = PorterStemmer()"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7O_BbSFp83Ig","outputId":"c6676d41-f120-43c0-e887-50168f1cc7ce","executionInfo":{"status":"ok","timestamp":1712448313229,"user_tz":-330,"elapsed":2119,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = '/content/drive/MyDrive/Ashik/Thesis/'\n","df = pd.read_csv(f'{path}data/dataset-2.csv')\n","df.shape"],"metadata":{"id":"zno9pNff707T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712448350537,"user_tz":-330,"elapsed":37310,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}},"outputId":"1d0c20e5-4b8e-44b3-94b1-fa0953c6d97d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["(2111, 3)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from nltk.stem.porter import PorterStemmer\n","import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","import gensim.downloader as api\n","from nltk import sent_tokenize\n","from gensim.utils import simple_preprocess\n","\n","\n","ps = PorterStemmer()\n","\n","def clean_text(text):\n","    text = BeautifulSoup(text, \"html.parser\").get_text()\n","    text = re.sub(r'[^a-zA-Z]', ' ', text)\n","    text = text.lower()\n","    tokens = word_tokenize(text)\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n","    cleaned_text = ' '.join(stemmed_tokens)\n","    return cleaned_text\n","\n","df['description'] = df ['description'].apply(clean_text)\n","text = df['description']\n","category = df['category']\n","\n","wv = api.load('word2vec-google-news-300')\n","\n","corpus = []\n","for i in range(len(text)):\n","    x = text[i]\n","    temp = re.sub('[^a-zA-Z0-9]',' ',x)  # Removing Unnecessary words\n","    temp = temp.lower()  # Converting them to lowercase\n","    temp = temp.split()  # Splitting the words\n","\n","    # Appending words not found in stopwords\n","    temp = [ps.stem(word) for word in temp if not word in stopwords.words('english')]\n","    temp = ' '.join(temp)\n","    corpus.append(temp)\n","\n","\n","words = []\n","for sent in corpus:\n","  sent_token = sent_tokenize(sent) ## Tokenization\n","  for sent in sent_token:\n","    words.append(simple_preprocess(sent)) ## Lowering the words\n","\n","\n","model = gensim.models.Word2Vec(words,window=5,min_count=2)\n","w2v_model = model\n","\n","sent_tokenize(corpus[0])\n","\n","\n","# Now lets conver the output category into numeric by encoding them\n","y = pd.get_dummies(df['category'],columns=df[\"category\"]).values\n","\n","\n","\n","# Split the data\n","from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test = train_test_split(corpus,y,test_size=0.3,random_state=24)\n","\n","\n","train_words = []\n","for sent in x_train:\n","  sent_token = sent_tokenize(sent) ## Tokenization\n","  for sent in sent_token:\n","    train_words.append(simple_preprocess(sent)) ## Lowering the words\n","\n","model = gensim.models.Word2Vec(train_words,window=5,min_count=2)\n","\n","\n","# Define the tokenizer\n","from keras.preprocessing.text import Tokenizer\n","maxlen = 3000\n","tokenizer = Tokenizer(num_words=50000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',lower=True)\n","tokenizer.fit_on_texts(x_train)\n","\n","word_index = tokenizer.word_index\n","\n","\n","x_train_sequences = tokenizer.texts_to_sequences(x_train)\n","x_test_sequences = tokenizer.texts_to_sequences(x_test)\n","\n","print(len(x_train_sequences[0]),len(x_train_sequences[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahTeotgiiisG","executionInfo":{"status":"ok","timestamp":1712448714473,"user_tz":-330,"elapsed":363952,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}},"outputId":"31dede2a-7b62-4b4a-da38-d66815a17b6f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-442ba4cda239>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  text = BeautifulSoup(text, \"html.parser\").get_text()\n","<ipython-input-5-442ba4cda239>:12: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  text = BeautifulSoup(text, \"html.parser\").get_text()\n"]},{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n","68 41\n"]}]},{"cell_type":"code","source":["# Since the number representation of the text are not the same, we need to pad them in order to make them same\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import TimeDistributed\n","from keras.layers import Bidirectional\n","from keras import backend as K\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","\n","x_train = pad_sequences(x_train_sequences, maxlen=3000,padding='pre')\n","x_test = pad_sequences(x_test_sequences,maxlen=3000,padding='pre')\n","\n","\n","model=Sequential()\n","model.add(Embedding(50000,100,input_length=3000))  # Input as the number of words (vocabulary size)\n","model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))) #Softmax for multiclass\n","model.add(Dense(5, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy',f1_m,precision_m, recall_m])"],"metadata":{"id":"gtG8X35IkIz_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712448716167,"user_tz":-330,"elapsed":1707,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}},"outputId":"9d3d7946-305a-4e2b-d66a-7a03a39c1e4a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"code","source":["history = model.fit(x_train,y_train, epochs=7, batch_size=64,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jbp7Y3m6ijc3","outputId":"5397966c-c4f9-4071-8d31-3d48deb634cb","executionInfo":{"status":"ok","timestamp":1712451822261,"user_tz":-330,"elapsed":3106102,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/7\n","21/21 [==============================] - 441s 21s/step - loss: 1.4705 - accuracy: 0.3679 - f1_m: 0.1534 - precision_m: 0.4899 - recall_m: 0.0959 - val_loss: 1.2643 - val_accuracy: 0.4865 - val_f1_m: 0.1896 - val_precision_m: 0.8333 - val_recall_m: 0.1104\n","Epoch 2/7\n","21/21 [==============================] - 439s 21s/step - loss: 1.1479 - accuracy: 0.5237 - f1_m: 0.4412 - precision_m: 0.7555 - recall_m: 0.3307 - val_loss: 0.9774 - val_accuracy: 0.5946 - val_f1_m: 0.5492 - val_precision_m: 0.7720 - val_recall_m: 0.4292\n","Epoch 3/7\n","21/21 [==============================] - 441s 21s/step - loss: 0.8932 - accuracy: 0.6388 - f1_m: 0.5756 - precision_m: 0.8095 - recall_m: 0.4487 - val_loss: 0.8248 - val_accuracy: 0.6824 - val_f1_m: 0.5893 - val_precision_m: 0.8410 - val_recall_m: 0.4562\n","Epoch 4/7\n","21/21 [==============================] - 442s 21s/step - loss: 0.6523 - accuracy: 0.7344 - f1_m: 0.7013 - precision_m: 0.8697 - recall_m: 0.5911 - val_loss: 0.7839 - val_accuracy: 0.7162 - val_f1_m: 0.6482 - val_precision_m: 0.8392 - val_recall_m: 0.5396\n","Epoch 5/7\n","21/21 [==============================] - 449s 21s/step - loss: 0.5002 - accuracy: 0.8337 - f1_m: 0.7945 - precision_m: 0.8763 - recall_m: 0.7299 - val_loss: 0.8045 - val_accuracy: 0.6757 - val_f1_m: 0.6848 - val_precision_m: 0.7191 - val_recall_m: 0.6542\n","Epoch 6/7\n","21/21 [==============================] - 445s 21s/step - loss: 0.3950 - accuracy: 0.8646 - f1_m: 0.8593 - precision_m: 0.8924 - recall_m: 0.8290 - val_loss: 0.7052 - val_accuracy: 0.7500 - val_f1_m: 0.7302 - val_precision_m: 0.7443 - val_recall_m: 0.7167\n","Epoch 7/7\n","21/21 [==============================] - 448s 21s/step - loss: 0.2861 - accuracy: 0.9105 - f1_m: 0.9103 - precision_m: 0.9230 - recall_m: 0.8981 - val_loss: 0.7944 - val_accuracy: 0.7095 - val_f1_m: 0.6634 - val_precision_m: 0.7023 - val_recall_m: 0.6302\n"]}]},{"cell_type":"code","source":["loss, accuracy, f1_score, precision, recall = model.evaluate(x_test, y_test, verbose=0)"],"metadata":{"id":"JIEA1Mrxijae","executionInfo":{"status":"ok","timestamp":1712451850516,"user_tz":-330,"elapsed":28268,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(loss, accuracy, f1_score, precision, recall)"],"metadata":{"id":"R5zbnMJFirTO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712451850517,"user_tz":-330,"elapsed":68,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}},"outputId":"78d57cf2-058e-497e-c0ca-70ff8696303a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7782526016235352 0.7176656126976013 0.7090528607368469 0.7273728251457214 0.6920672655105591\n"]}]},{"cell_type":"code","source":["# #bi-directional\n","# model=Sequential()\n","# model.add(Embedding(50000,100,input_length=3000))  # Input as the number of words (vocabulary size)\n","# model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2))) #Softmax for multiclass\n","# model.add(TimeDistributed(Dense(5, activation='softmax')))\n","# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', auroc])"],"metadata":{"id":"0Nd5s8IfPSmU","executionInfo":{"status":"ok","timestamp":1712451850517,"user_tz":-330,"elapsed":56,"user":{"displayName":"cdv dev","userId":"01785238182428910549"}}},"execution_count":10,"outputs":[]}]}